{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# Adapted from TinyML edX\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30GC2JEwL2xj"
   },
   "source": [
    "## Start by downloading some data into the Colab Instance for Training!\n",
    "We are going to be building a CV model to differentiate between images of horses and images of humans. You'll note the data comes from [Laurence Moroney](https://laurencemoroney.com/) of Google who developed the original Colab for the TinML edX course and is a wealth of knowledge on TensorFlow (Google's ML library) which we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXZT2UsyIVe_",
    "outputId": "48b98366-cc04-4cc3-ce9b-7bad634a1e92"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
    "    -O /tmp/horse-or-human.zip\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
    "    -O /tmp/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = '/tmp/horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/horse-or-human')\n",
    "local_zip = '/tmp/validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_-A0q1QL2xk",
    "outputId": "95cc9928-c0b2-46b8-a996-272c0ce7959e"
   },
   "outputs": [],
   "source": [
    "# Directory with our training horse pictures\n",
    "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
    "# Directory with our training human pictures\n",
    "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
    "# Directory with our training horse pictures\n",
    "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
    "# Directory with our training human pictures\n",
    "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\n",
    "train_horse_names = os.listdir('/tmp/horse-or-human/horses')\n",
    "print(train_horse_names[:10])\n",
    "train_human_names = os.listdir('/tmp/horse-or-human/humans')\n",
    "print(train_human_names[:10])\n",
    "validation_horse_hames = os.listdir('/tmp/validation-horse-or-human/horses')\n",
    "print(validation_horse_hames[:10])\n",
    "validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\n",
    "print(validation_human_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvfZg3LQbD-5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DFi8qSIL2xl"
   },
   "source": [
    "## Define your model and optimizer\n",
    "We are just going to use a few convolutional (+ pooling) layers and then some standard dense perceptron layers (think AlexNet but way smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PixZ2s5QbYQ3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 100x100 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCO9TvtOPtH7",
    "outputId": "4f18e83b-5231-4f9d-dcb7-c0268047e029"
   },
   "outputs": [],
   "source": [
    "# Note that even this small model has a LOT of parameters to train!\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "# Adam is the SGD with an adaptive learning rate (alpha) and is pretty standard\n",
    "# state-of-the-art for training. And binary_crossentropy is good for telling\n",
    "# apart two distinct classes -- you can find others here for more classes\n",
    "# https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKYbRjx8L2xl"
   },
   "source": [
    "## Now when we organize the data into Generators\n",
    "This is just some tensorflow shorthard that automagically does some data\n",
    "augmentation for you. Aka it warps and manipulates the images during\n",
    "training to make your final model more robust to real world noise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClebU9NJg99G",
    "outputId": "85bd42d0-d012-45d8-e63f-bff24fd8184b"
   },
   "outputs": [],
   "source": [
    "# All images will be augmented wiht the full list of augmentation techniques below\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest'\n",
    "      )\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(100, 100),  # All images will be resized to 300x300\n",
    "        batch_size=128,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '/tmp/validation-horse-or-human',\n",
    "        target_size=(100, 100),\n",
    "        class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcGH0D3CL2xm"
   },
   "source": [
    "## Train your model with the new augmented data\n",
    "Since we are using data augmentation this training process will take a bit longer. However, you'll find that the results are much better (usually)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb1_lgobv81m",
    "outputId": "e11af0c1-829d-4ee3-f6f9-5dc99961f77a"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=40,\n",
    "      verbose=1,\n",
    "      validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "## Test the Model!\n",
    "\n",
    "Upload some images of horses or of humans. Did the model get them right? Did the extra data augmentation help the model generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "DoWp43WxJDNT",
    "outputId": "14abd69d-a153-4e82-eda8-e8c0dd2eb66f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = '/content/' + fn\n",
    "  img = tf.keras.utils.load_img(path, target_size=(100, 100))\n",
    "  x = tf.keras.utils.img_to_array(img)\n",
    "  x = x / 255.0\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  image_tensor = np.vstack([x])\n",
    "  classes = model.predict(image_tensor)\n",
    "  print(classes)\n",
    "  print(classes[0])\n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" is a human\")\n",
    "  else:\n",
    "    print(fn + \" is a horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzVJDupFL2xm"
   },
   "source": [
    "## Finally lets visualize some of the layers\n",
    "This may provide some intution for what the CNN is doing -- or it might not! Its hard to tell what's going on as you get deeper and deeper into the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xV_9ZT7CL2xn",
    "outputId": "7fdf8379-ee80-4a08-e4fd-9c193b04b7da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "# uncomment the following line if you want to pick the Xth human file manually\n",
    "# img_path = human_img_files[0]\n",
    "\n",
    "img = tf.keras.utils.load_img(img_path, target_size=(100, 100))  # this is a PIL image\n",
    "x = tf.keras.utils.img_to_array(img)  # Numpy array with shape (100, 100, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 100, 100, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255.0\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "    n_features = min(n_features,5) # limit to 5 features for easier viewing\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      # Postprocess the feature to make it visually palatable\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "      # We'll tile each filter into this big horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    #plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Clean Up\n",
    "\n",
    "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
